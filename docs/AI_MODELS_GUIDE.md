# Руководство по AI моделям и автоматическому Fallback

## Обзор

Система использует несколько AI моделей с автоматическим переключением (fallback) и поддержкой генерации очень длинных ответов через chunking.

## Архитектура

```
┌─────────────────────────────────────────────┐
│   Входящий запрос пользователя              │
├─────────────────────────────────────────────┤
│   Анализ сложности запроса                  │
│   (ключевые слова, длина, тип задачи)      │
├─────────────────────────────────────────────┤
│   Выбор модели:                             │
│   1. GPT-5-thinking (сложные задачи)        │
│   2. GPT-5 (основная модель)                │
│   3. GPT-4.1 (fallback)                     │
├─────────────────────────────────────────────┤
│   Генерация с chunking при необходимости    │
│   (автоматическое разделение на части)      │
├─────────────────────────────────────────────┤
│   Автоматический fallback при ошибках       │
│   (rate limit, timeout, server error)       │
└─────────────────────────────────────────────┘
```

## Конфигурация моделей

### GPT-5-thinking (Приоритет: Наивысший для сложных задач)

**Когда используется:**
- Запросы с ключевыми словами: "проанализируй", "стратегия", "риски", "подготовь", "жалобу", "апелляцию"
- Длинные запросы (> 200 символов)
- Запросы на большой объем текста (> 5000 слов)

**Параметры:**
- Модель: `gpt-5-thinking`
- Контекст: 400,000 токенов
- Ответ: до 32,000 токенов
- Reasoning effort: `high`
- Verbosity: `high`
- Temperature: 0.7

**Идеально для:**
- Подготовка юридических документов (жалобы, иски, ходатайства)
- Глубокий анализ дел и ситуаций
- Разработка стратегий защиты
- Анализ рисков и последствий

---

### GPT-5 (Приоритет: Основная модель)

**Когда используется:**
- Все остальные запросы по умолчанию
- Средние по сложности задачи
- Консультации и разъяснения

**Параметры:**
- Модель: `gpt-5`
- Контекст: 400,000 токенов
- Ответ: до 32,000 токенов
- Reasoning effort: `medium`
- Verbosity: `medium`
- Temperature: 0.7

**Идеально для:**
- Общие юридические консультации
- Ответы на вопросы
- Анализ документов
- Средние по объему задачи

---

### GPT-4.1 Turbo (Приоритет: Fallback модель)

**Когда используется:**
- Когда GPT-5 недоступна (rate limit, ошибка)
- Автоматический fallback при проблемах

**Параметры:**
- Модель: `gpt-4-turbo-2024-04-09`
- Контекст: 128,000 токенов
- Ответ: до 16,000 токенов
- Temperature: 0.7

**Идеально для:**
- Резервное обеспечение доступности сервиса
- Быстрые ответы при недоступности GPT-5

---

## Автоматический Fallback

Система автоматически переключается на резервную модель при:

1. **Rate Limit** - превышен лимит запросов к API
2. **Timeout** - модель не отвечает вовремя
3. **Server Error** - ошибки сервера OpenAI (500, 503)
4. **Недоступность модели** - модель временно недоступна
5. **Ошибки квоты** - недостаточно средств/квоты

### Порядок fallback:

```
GPT-5-thinking → GPT-5 → GPT-4.1
     ↓              ↓         ↓
  (если не сработала каждая следующая)
```

---

## Chunking (разделение длинных ответов)

Для генерации очень длинных ответов (10,000+ слов) система автоматически:

1. **Генерирует первую часть** до максимума токенов
2. **Проверяет обрезку** (`finish_reason === 'length'`)
3. **Запрашивает продолжение** автоматически
4. **Объединяет части** в единый ответ
5. **Повторяет** до 5 раз или до завершения

### Автоматический chunking активируется когда:

- В запросе указано: "не менее X слов/токенов" где X >= 3000
- Запрос содержит: "большой", "длинный", "подробный", "детальный", "глубокий"
- Запрос явно просит большой объем текста

**Пример:**
```
Пользователь: "Подготовь дополнения к апелляционной жалобе. Не менее 10000 слов"

Система:
1. Генерирует часть 1 (32K токенов)
2. Обнаруживает обрезку
3. Запрашивает: "Продолжи документ..."
4. Генерирует часть 2 (32K токенов)
5. Обнаруживает обрезку
6. Запрашивает продолжение...
7. И так далее до завершения
```

---

## Метаданные ответа

Каждый ответ содержит метаданные:

```typescript
{
  modelUsed: "gpt-5-thinking",       // Использованная модель
  fallbackOccurred: false,            // Был ли fallback
  fallbackReason: undefined,          // Причина fallback (если был)
  chunksCount: 3,                     // Количество частей
  totalTokens: 87532,                 // Всего использовано токенов
  finishReason: "stop",               // Причина завершения
  responseTimeMs: 45230               // Время генерации в мс
}
```

Метаданные логируются в консоль браузера для отладки.

---

## Примеры использования

### Простой вопрос (GPT-5)
```
Пользователь: "Что такое апелляционная жалоба?"
Система: Использует GPT-5, быстрый ответ, без chunking
```

### Сложный анализ (GPT-5-thinking)
```
Пользователь: "Проанализируй приговор и подготовь стратегию защиты с оценкой всех рисков"
Система: Использует GPT-5-thinking, глубокий анализ
```

### Длинный документ (GPT-5-thinking + chunking)
```
Пользователь: "Подготовь дополнения к апелляционной жалобе. Не менее 10000 слов"
Система: 
- Использует GPT-5-thinking
- Автоматический chunking (3-5 частей)
- Полный документ на выходе
```

### Fallback сценарий
```
Пользователь: "Какие у меня права при задержании?"
Система:
1. Пытается GPT-5
2. Получает rate_limit_exceeded
3. Автоматически переключается на GPT-4.1
4. Возвращает ответ
```

---

## Мониторинг и логирование

Все запросы логируются с метаданными:

```bash
[AI Service] Primary model selected: gpt-5-thinking
[AI Service] Fallback chain: gpt-5-thinking -> gpt-5 -> gpt-4.1
[Chunker] Starting generation with model: gpt-5-thinking, max_tokens: 32000
[Chunker] Received chunk 1: 125430 chars, finish_reason: length
[Chunker] Response was truncated. Requesting continuation...
[Chunker] Received chunk 2: 118920 chars, finish_reason: length
[Chunker] Response was truncated. Requesting continuation...
[Chunker] Received chunk 3: 95340 chars, finish_reason: stop
[Chunker] Final result: 3 chunks, 339690 chars, 87532 tokens
[AI Service] Success with model: gpt-5-thinking
AI Response metadata: {
  modelUsed: 'gpt-5-thinking',
  fallbackOccurred: false,
  chunksCount: 3,
  totalTokens: 87532,
  finishReason: 'stop',
  responseTimeMs: 45230
}
```

---

## Настройка

### Изменение приоритета моделей

Редактируйте `lib/model-config.ts`:

```typescript
export const MODEL_CONFIGS: Record<ModelName, ModelConfig> = {
  'gpt-5-thinking': {
    priority: 0, // Наивысший приоритет для reasoning
  },
  'gpt-5': {
    priority: 1, // Основная модель
  },
  'gpt-4.1': {
    priority: 2, // Fallback
  },
}
```

### Изменение ключевых слов для reasoning

Редактируйте `lib/model-config.ts`:

```typescript
export const REASONING_KEYWORDS = [
  'проанализируй',
  'стратегия',
  'риски',
  // ... добавьте свои
];
```

### Изменение лимитов chunking

Редактируйте `lib/response-chunker.ts`:

```typescript
const MAX_CONTINUATION_ATTEMPTS = 5; // Максимум частей
```

---

## Переменные окружения

Убедитесь что установлен API ключ OpenAI:

```bash
OPENAI_API_KEY=sk-...
```

---

## Тестирование

После запуска системы протестируйте:

1. **Простой запрос:**
   ```
   "Что такое УК РФ?"
   ```
   Ожидается: GPT-5, быстрый ответ

2. **Сложный запрос:**
   ```
   "Проанализируй стратегию защиты по делу о мошенничестве"
   ```
   Ожидается: GPT-5-thinking, детальный анализ

3. **Длинный документ:**
   ```
   "Подготовь апелляционную жалобу. Не менее 10000 слов"
   ```
   Ожидается: GPT-5-thinking + chunking (2-4 части)

4. **Fallback (симулировать недоступность):**
   - Временно отключите API ключ
   - Проверьте что система переключается на fallback

---

## Структура файлов

```
lib/
├── model-config.ts        # Конфигурация всех моделей
├── ai-service.ts          # Сервис с fallback логикой
├── response-chunker.ts    # Обработка длинных ответов
└── types.ts               # TypeScript типы

app/api/chat/
└── route.ts               # API endpoint использующий AI service

components/
└── chat-page-client.tsx   # Frontend с логированием метаданных
```

---

## Известные ограничения

1. **Максимум 5 chunks** - для предотвращения бесконечных циклов
2. **GPT-5 модель** - название модели может измениться при релизе OpenAI
3. **Стоимость** - длинные ответы потребляют больше токенов
4. **Время** - генерация больших документов может занять 30-60 секунд

---

## Поддержка

При проблемах проверьте:
1. Логи в консоли браузера (метаданные ответов)
2. Логи сервера (Node.js консоль)
3. Метаданные ответа (модель, fallback, chunks)

---

## Обновления

**v1.0** - Базовая реализация с GPT-5, GPT-5-thinking, GPT-4.1 fallback
- ✅ Автоматический выбор модели
- ✅ Fallback на резервные модели
- ✅ Chunking для длинных ответов
- ✅ Полное логирование
- ✅ Метаданные в ответах

---

## Roadmap

- [ ] Streaming ответов (real-time генерация)
- [ ] Кеширование ответов
- [ ] Rate limiting на клиенте
- [ ] Визуальный индикатор chunking в UI
- [ ] Статистика использования моделей
- [ ] A/B тестирование моделей




